# ğŸ” Job Market Analysis Agentic AI Application

An intelligent AI agent that automatically discovers, downloads, preprocesses, and analyzes job market datasets from various sources. Built specifically for analyzing the [ABS Job Vacancies data](https://www.abs.gov.au/statistics/labour/jobs/job-vacancies-australia/may-2025) and other job market datasets.

## âœ¨ Features

- **ğŸ” URL-based Dataset Discovery**: Automatically finds and downloads datasets from provided URLs
- **ğŸ“Š Multi-format Support**: Handles CSV, XLSX, XLS, JSON, and HTML tables
- **ğŸ§¹ Intelligent Preprocessing**: Automatically cleans, validates, and prepares data for analysis
- **ğŸ§  Advanced Analytics**: Provides comprehensive job market insights and trends
- **ğŸŒ Modern Web Interface**: Built with FastAPI and Streamlit
- **ğŸ¤– Agentic AI**: Multiple specialized AI agents working together
- **ğŸ“Š Universal CSV Output**: All preprocessed data saved as CSV regardless of original format
- **ğŸ“ Professional Logging**: Comprehensive logging system with file output and rotation

## ğŸš€ Quick Start

### Option 1: One-Click Startup (Recommended)

```bash
# Start the backend
python main.py

# In a new terminal, start the frontend
streamlit run streamlit_app.py
```

### Option 2: Manual Startup

1. **Install Dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

2. **Run the Application**:

   ```bash
   # Terminal 1: FastAPI Backend
   uvicorn main:app --reload

   # Terminal 2: Streamlit Frontend
   streamlit run streamlit_app.py
   ```

3. **Access the Application**:
   - ğŸŒ **FastAPI Backend**: http://localhost:8000
   - ğŸ“š **API Docs**: http://localhost:8000/docs
   - ğŸ¨ **Streamlit Frontend**: http://localhost:8501

## ğŸ¯ Usage

### For ABS Job Vacancies Data

1. **Enter the ABS URL**: `https://www.abs.gov.au/statistics/labour/jobs/job-vacancies-australia/may-2025`
2. **Agent automatically discovers** the 4 key datasets:
   - Table 1: Job vacancies by states and territories
   - Table 2: Private sector job vacancies by states and territories
   - Table 3: Public sector job vacancies by states and territories
   - Table 4: Job vacancies by industry across Australia
3. **Downloads and preprocesses** the data automatically
4. **Analyzes** key metrics, trends, and insights

### For Other Datasets

1. **Provide any URL** containing job market data
2. **Upload your own files** (CSV, XLSX, XLS, JSON)
3. **Let the AI agents** handle the rest automatically

## ğŸ—ï¸ Project Structure

```
job-vacancies-agent/
â”œâ”€â”€ ğŸ”§ main.py               # FastAPI backend
â”œâ”€â”€ ğŸ¨ streamlit_app.py      # Streamlit frontend
â”œâ”€â”€ ğŸ¤– agents/
â”‚   â”œâ”€â”€ data_discovery.py    # URL scraping and dataset discovery
â”‚   â”œâ”€â”€ data_processor.py    # Data preprocessing pipeline
â”‚   â””â”€â”€ analyzer.py          # Analysis and insights engine
â”œâ”€â”€ ğŸ› ï¸ utils/
â”‚   â””â”€â”€ logger.py            # Professional logging system
â”œâ”€â”€ ğŸ“ data/                 # Downloaded datasets
â”‚   â”œâ”€â”€ raw/                 # Original datasets
â”‚   â””â”€â”€ preprocessed/        # Clean, processed data (CSV)

â””â”€â”€ ğŸ“ logs/                 # Professional log files
```

## ğŸ¤– AI Agents Architecture

### 1. **Data Discovery Agent**

- Scrapes URLs to find downloadable datasets
- Identifies CSV, XLSX, and other data sources
- Extracts HTML tables when direct downloads aren't available

### 2. **Data Processing Agent**

- Cleans and validates raw data
- Handles missing values intelligently
- Optimizes data types and removes outliers
- Generates data quality reports

### 3. **Analysis Agent**

- Performs comprehensive job market analysis
- Identifies trends, patterns, and insights
- Generates actionable recommendations
- Analyzes geographic, industry, and sector distributions

## ğŸ“Š Supported Data Sources

- **Australian Bureau of Statistics (ABS)** - Primary focus
- **Government datasets** and reports
- **CSV files** and spreadsheets
- **Excel files** (XLSX, XLS)
- **JSON data** and APIs
- **HTML tables** from web pages

## ğŸ”Œ API Endpoints

### Core Endpoints

- `POST /analyze-url` - Analyze datasets from URL
- `POST /upload-dataset` - Upload and analyze file
- `GET /datasets` - List all processed datasets
- `GET /dataset/{name}` - Get specific dataset info
- `GET /analysis/{name}` - Get analysis results
- `GET /health` - System health check

### Example API Usage

```python
import requests

# Analyze ABS URL
response = requests.post("http://localhost:8000/analyze-url",
                        json={"url": "https://www.abs.gov.au/statistics/labour/jobs/job-vacancies-australia/may-2025"})

# Get results
data = response.json()
print(f"Discovered {data['datasets_discovered']} datasets")
```

## ğŸ¨ Frontend Options

### 1. **Streamlit Interface** (Recommended)

- Interactive data exploration
- User-friendly controls
- Access at: http://localhost:8501

### 2. **FastAPI Web Interface**

- Clean, modern design
- Direct API access
- Responsive layout
- Access at: http://localhost:8000

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file for custom settings:

```env
DEBUG=True
LOG_LEVEL=INFO
MAX_FILE_SIZE=100MB
```

### Custom Analysis Rules

Modify `agents/analyzer.py` to add custom analysis logic for your specific use cases.

## ğŸš€ Deployment

### Local Development

```bash
# Start backend
python main.py

# Start frontend (in new terminal)
streamlit run streamlit_app.py
```

### Production Deployment

```bash
# Install production dependencies
pip install -r requirements.txt

# Start with production server
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker

# Or use Docker
docker build -t job-market-ai .
docker run -p 8000:8000 job-market-ai
```

## ğŸ“ˆ Example Analysis Output

The system automatically generates:

- **ğŸ“Š Overview Metrics**: Total vacancies, data quality, coverage
- **ğŸ“ˆ Trend Analysis**: Growth patterns, seasonal variations
- **ğŸ—ºï¸ Geographic Insights**: State/territory distributions
- **ğŸ­ Industry Analysis**: Sector breakdowns and comparisons
- **ğŸ’¡ Key Insights**: Automated discovery of important patterns
- **ğŸ¯ Recommendations**: Actionable business intelligence

## ğŸ“Š Data Processing Pipeline

### **Overview**

The system automatically processes all raw data formats and converts them to standardized CSV format for consistency and analysis.

### **Raw Data Storage (`data/raw/`)**

- **Original files** in their native format (CSV, XLSX, XLS, JSON)
- **Preserved exactly** as downloaded or uploaded
- **No modifications** to original data

### **Preprocessed Data Storage (`data/preprocessed/`)**

- **All data converted to CSV** regardless of original format
- **Cleaned and standardized** column names and data types
- **Missing value handling** and outlier treatment
- **Data quality improvements** applied consistently

### **Multi-Sheet Excel Handling**

- **Each sheet processed individually** and saved as separate CSV
- **Sheet-specific folders** created for organization
- **Combined CSV file** with all sheets merged
- **Summary documentation** of all sheets and their properties

### **File Structure Example**

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ dataset1.xlsx          # Original Excel file
â”‚   â”œâ”€â”€ dataset2.csv           # Original CSV file
â”‚   â””â”€â”€ dataset3.json          # Original JSON file
â””â”€â”€ preprocessed/
    â”œâ”€â”€ dataset1_preprocessed.csv           # Main processed CSV
    â”œâ”€â”€ dataset1_all_sheets_combined.csv   # All sheets merged
    â”œâ”€â”€ dataset1_sheets/                   # Individual sheets
    â”‚   â”œâ”€â”€ Sheet1.csv
    â”‚   â”œâ”€â”€ Sheet2.csv
    â”‚   â””â”€â”€ _sheets_summary.txt
    â”œâ”€â”€ dataset2_preprocessed.csv           # Processed CSV
    â””â”€â”€ dataset3_preprocessed.csv           # Processed CSV
```

## ğŸ“ Professional Logging System

### **Overview**

The system includes a comprehensive logging framework that tracks all operations, performance metrics, and system events.

### **Log Files Structure**

```
logs/
â”œâ”€â”€ main_all.log              # All logs from main application
â”œâ”€â”€ main_errors.log           # Error logs only
â”œâ”€â”€ main_performance.log      # Performance metrics
â”œâ”€â”€ data_discovery_all.log    # Data discovery agent logs
â”œâ”€â”€ data_processor_all.log    # Data processing agent logs
â”œâ”€â”€ analyzer_all.log          # Analysis agent logs
```

### **Log Levels**

- **DEBUG**: Detailed debugging information
- **INFO**: General operational messages
- **WARNING**: Warning messages
- **ERROR**: Error messages
- **CRITICAL**: Critical system failures

### **Specialized Logging**

- **Performance Logging**: Tracks execution times for operations
- **Data Operation Logging**: Records dataset processing activities
- **API Request Logging**: Monitors API endpoint usage and performance
- **Error Tracking**: Detailed error logging with context

### **Log Rotation**

- **File Size Limits**: 10MB for general logs, 5MB for error logs
- **Backup Count**: Keeps 5 backup files for general logs, 3 for error logs
- **Automatic Management**: Handles log file rotation automatically

### **Usage Example**

```python
from utils.logger import get_logger

# Get logger for your component
logger = get_logger("my_component")

# Log different types of information
logger.info("Operation started")
logger.performance("DATA_PROCESSING", 2.5, "Processed 1000 rows")
logger.error("Failed to process dataset")
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- Australian Bureau of Statistics for providing comprehensive job market data
- FastAPI and Streamlit communities for excellent frameworks
